package raft

//
// Raft tests.
//
// We will use the original test_test.go to test your code for grading.
// so, while you can modify this code to help you debug, please
// test with the original before submitting.
//

import (
	"fmt"
	"math/rand"
	"sync"
	"testing"
	"time"
)

// The tester generously allows solutions to complete elections in one second
// (much more than the paper's range of timeouts).
const RaftElectionTimeout = 1000 * time.Millisecond

func TestInitialElection3A(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3A): initial election")

	// Is a leader elected?
	cfg.checkOneLeader()

	// Sleep a bit to avoid racing with followers learning of the
	// election, then check that all peers agree on the term.
	time.Sleep(50 * time.Millisecond)
	term1 := cfg.checkTerms()
	if term1 < 1 {
		t.Fatalf("term is %v, but should be at least 1", term1)
	}

	// Does the leader+term stay the same if there is no network failure?
	time.Sleep(2 * RaftElectionTimeout)
	term2 := cfg.checkTerms()
	if term1 != term2 {
		fmt.Printf("warning: term changed even though there were no failures")
	}

	// There should still be a leader.
	cfg.checkOneLeader()

	cfg.end()
}

func TestReElection3A(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3A): election after network failure")

	leader1 := cfg.checkOneLeader()

	// If the leader disconnects, a new one should be elected.
	cfg.disconnect(leader1)
	cfg.checkOneLeader()

	// If the old leader rejoins, that shouldn't disturb the new leader.
	cfg.connect(leader1)
	leader2 := cfg.checkOneLeader()

	// If there's no quorum, no leader should be elected.
	cfg.disconnect(leader2)
	cfg.disconnect((leader2 + 1) % servers)
	time.Sleep(2 * RaftElectionTimeout)
	cfg.checkNoLeader()

	// If a quorum arises, it should elect a leader.
	cfg.connect((leader2 + 1) % servers)
	cfg.checkOneLeader()

	// Re-join of last node shouldn't prevent leader from existing.
	cfg.connect(leader2)
	cfg.checkOneLeader()

	cfg.end()
}

func TestManyElections3A(t *testing.T) {
	servers := 7
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3A): multiple elections")

	cfg.checkOneLeader()

	iters := 10
	for ii := 1; ii < iters; ii++ {
		// Disconnect three nodes.
		i1 := rand.Int() % servers
		i2 := rand.Int() % servers
		i3 := rand.Int() % servers
		cfg.disconnect(i1)
		cfg.disconnect(i2)
		cfg.disconnect(i3)

		// Rither the current leader should still be alive,
		// or the remaining four should elect a new one.
		cfg.checkOneLeader()

		cfg.connect(i1)
		cfg.connect(i2)
		cfg.connect(i3)
	}

	cfg.checkOneLeader()

	cfg.end()
}

// done
func TestBasicAgree3B(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): basic agreement")

	iters := 3
	for index := 1; index < iters+1; index++ {
		nd, _ := cfg.nCommitted(index)
		if nd > 0 {
			t.Fatalf("some have committed before Start()")
		}

		xindex := cfg.one(index*100, servers, false)
		if xindex != index {
			t.Fatalf("got index %v but expected %v", xindex, index)
		}
	}

	cfg.end()
}

// done
// Check, based on counting bytes of RPCs, that
// each command is sent to each peer just once.
func TestRPCBytes3B(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): RPC byte count")

	cfg.one(99, servers, false)
	bytes0 := cfg.bytesTotal()

	iters := 10
	var sent int64 = 0
	for index := 2; index < iters+2; index++ {
		cmd := randstring(5000)
		xindex := cfg.one(cmd, servers, false)
		if xindex != index {
			t.Fatalf("got index %v but expected %v", xindex, index)
		}
		sent += int64(len(cmd))
	}

	bytes1 := cfg.bytesTotal()
	got := bytes1 - bytes0
	expected := int64(servers) * sent
	if got > expected+50000 {
		t.Fatalf("too many RPC bytes; got %v, expected %v", got, expected)
	}

	cfg.end()
}

// failed
func TestFailAgree3B(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): agreement despite follower disconnection")

	cfg.one(101, servers, false)

	// Disconnect one follower from the network.
	leader := cfg.checkOneLeader()
	disconnected_node := (leader + 1) % servers
	cfg.disconnect(disconnected_node)
	// cfg.disconnect((leader + 1) % servers)
	fmt.Printf("Node %d gets disconnected\n", disconnected_node)

	// The leader and remaining follower should be
	// able to agree despite the disconnected follower.
	cfg.one(102, servers-1, false)
	cfg.one(103, servers-1, false)
	time.Sleep(RaftElectionTimeout)
	cfg.one(104, servers-1, false)
	cfg.one(105, servers-1, false)

	// Re-connect.
	cfg.connect((leader + 1) % servers)

	// The full set of servers should preserve previous
	// agreements, and be able to agree on new commands.
	cfg.one(106, servers, true)
	time.Sleep(RaftElectionTimeout)
	cfg.one(107, servers, true)

	cfg.end()
}

func TestFailNoAgree3B(t *testing.T) {
	servers := 5
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): no agreement if too many followers disconnect")

	cfg.one(10, servers, false)

	// 3 of 5 followers disconnect.
	leader := cfg.checkOneLeader()
	cfg.disconnect((leader + 1) % servers)
	cfg.disconnect((leader + 2) % servers)
	cfg.disconnect((leader + 3) % servers)

	index, _, ok := cfg.rafts[leader].Start(20)
	if ok != true {
		t.Fatalf("leader rejected Start()")
	}
	if index != 2 {
		t.Fatalf("expected index 2, got %v", index)
	}

	time.Sleep(2 * RaftElectionTimeout)

	n, _ := cfg.nCommitted(index)
	if n > 0 {
		t.Fatalf("%v committed but no majority", n)
	}

	// Repair.
	cfg.connect((leader + 1) % servers)
	cfg.connect((leader + 2) % servers)
	cfg.connect((leader + 3) % servers)

	// The disconnected majority may have chosen a leader from
	// among their own ranks, forgetting index 2.
	leader2 := cfg.checkOneLeader()
	index2, _, ok2 := cfg.rafts[leader2].Start(30)
	if ok2 == false {
		t.Fatalf("leader2 rejected Start()")
	}
	if index2 < 2 || index2 > 4 {
		t.Fatalf("unexpected index %v", index2)
	}

	cfg.one(1000, servers, true)

	cfg.end()
}

// yes
func TestConcurrentStarts3B(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): concurrent Start()s")

	var success bool
loop:
	for try := 0; try < 5; try++ {
		if try > 0 {
			// Give the solution some time to settle.
			time.Sleep(3 * time.Second)
		}

		leader := cfg.checkOneLeader()
		_, term, ok := cfg.rafts[leader].Start(1)
		if !ok {
			// Leader moved on really quickly.
			continue
		}

		iters := 5
		var wg sync.WaitGroup
		is := make(chan int, iters)
		for ii := 0; ii < iters; ii++ {
			wg.Add(1)
			go func(i int) {
				defer wg.Done()
				i, term1, ok := cfg.rafts[leader].Start(100 + i)
				if term1 != term {
					return
				}
				if ok != true {
					return
				}
				is <- i
			}(ii)
		}

		wg.Wait()
		close(is)

		for j := 0; j < servers; j++ {
			if t, _ := cfg.rafts[j].GetState(); t != term {
				// Term changed -- can't expect low RPC counts.
				continue loop
			}
		}

		failed := false
		cmds := []int{}
		for index := range is {
			sameTerm, e := cfg.wait(index, servers, term)
			if !sameTerm {
				// Peers have moved on to later terms so we
				// can't expect all Start()s to have succeeded.
				failed = true
				break
			}
			if e.valid {
				if ix, ok := e.command.(int); ok {
					cmds = append(cmds, ix)
				} else {
					t.Fatalf("value %v is not an int", e.command)
				}
			}
		}

		if failed {
			// Avoid leaking goroutines.
			go func() {
				for range is {
				}
			}()
			continue
		}

		for ii := 0; ii < iters; ii++ {
			x := 100 + ii
			ok := false
			for j := 0; j < len(cmds); j++ {
				if cmds[j] == x {
					ok = true
				}
			}
			if ok == false {
				t.Fatalf("cmd %v missing in %v", x, cmds)
			}
		}

		success = true
		break
	}

	if !success {
		t.Fatalf("term changed too often")
	}

	cfg.end()
}

// no
func TestRejoin3B(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): rejoin of partitioned leader")

	cfg.one(101, servers, true)

	// Leader network failure.
	leader1 := cfg.checkOneLeader()
	cfg.disconnect(leader1)

	fmt.Printf("First Leader, Node %d,  becomes disconnected\n", leader1)

	// Make old leader try to agree on some entries.
	cfg.rafts[leader1].Start(102)
	cfg.rafts[leader1].Start(103)
	cfg.rafts[leader1].Start(104)

	// New leader commits, also for index=2.
	cfg.one(103, 2, true)

	// New leader network failure.
	leader2 := cfg.checkOneLeader()
	fmt.Printf("Second Leader, Node %d becomes disconnected\n", leader2)

	// Old leader connected again.
	cfg.connect(leader1)
	fmt.Printf("First Leader, Node %d becomes reconnected\n", leader1)

	cfg.one(104, 2, true)

	// All together now.
	cfg.connect(leader2)
	fmt.Printf("Seconds Leader, Node %d becomes reconnected\n", leader2)

	cfg.one(105, servers, true)

	cfg.end()
}

// no
func TestBackup3B(t *testing.T) {
	servers := 5
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): leader backs up quickly over incorrect follower logs\n")

	cfg.one(rand.Int(), servers, true)
	fmt.Printf("Finished sending 1 commit to all servers. All servers finish commiting\n")

	// Put leader and one follower in a partition.
	leader1 := cfg.checkOneLeader()
	fmt.Printf("[Test] Leader1 elected: Node %d\n", leader1)

	disconnected_node1 := (leader1 + 2) % servers
	disconnected_node2 := (leader1 + 3) % servers
	disconnected_node3 := (leader1 + 4) % servers
	connected_node := (leader1 + 1) % servers

	cfg.disconnect(disconnected_node1)
	cfg.disconnect(disconnected_node2)
	cfg.disconnect(disconnected_node3)

	fmt.Printf("[Test] Node %d, %d, %d becomes disconnected (partitioned)\n", disconnected_node1, disconnected_node2, disconnected_node3)
	fmt.Printf("[Test] Node %d is still connected\n", connected_node)

	// Submit lots of commands that won't commit.
	for i := 0; i < 50; i++ {
		cfg.rafts[leader1].Start(rand.Int())
	}
	fmt.Printf("[Test] Finished sending 1st batch of 50 uncommitted commands to partitioned leader: %d. They should not be commited\n", leader1)

	time.Sleep(RaftElectionTimeout / 2)

	cfg.disconnect((leader1 + 0) % servers)
	cfg.disconnect((leader1 + 1) % servers)
	fmt.Printf("[Test] Node %d, %d becomes disconnected (2nd partitioned)\n", (leader1+0)%servers, (leader1+1)%servers)

	// Allow other partition to recover.
	cfg.connect(disconnected_node1)
	cfg.connect(disconnected_node2)
	cfg.connect(disconnected_node3)
	fmt.Printf("[Test] Node %d, %d, %d becomes re-connected\n", disconnected_node1, disconnected_node2, disconnected_node3)
	fmt.Printf("[Test] A leader should be elected among the 3 nodes \n")

	// Lots of successful commands to new group.
	for i := 0; i < 50; i++ {
		i := rand.Int()
		cfg.one(i, 3, true)
	}
	fmt.Printf("[Test] Finished sending 2nd batch of 50 commands to new majority group. These should be committed successfully.\n")

	// Now another partitioned leader and one follower
	leader2 := cfg.checkOneLeader()
	fmt.Printf("[Test] Leader2 elected: Node %d\n", leader2)

	other := (leader1 + 2) % servers
	if leader2 == other {
		other = (leader2 + 1) % servers
	}
	cfg.disconnect(other)
	fmt.Printf("[Test] Disconnected node %d. Leader, Node %d, and 1 more follower is still connected\n", other, leader2)

	// Lots more commands that won't commit.
	for i := 0; i < 50; i++ {
		cfg.rafts[leader2].Start(rand.Int())
	}
	fmt.Printf("[Test] Finished sending 3rd batch of 50 uncommitted commands to partitioned leader: %d. They should be not be commited\n", leader2)

	time.Sleep(RaftElectionTimeout / 2)

	// Bring original leader back to life.
	for i := 0; i < servers; i++ {
		cfg.disconnect(i)
	}
	cfg.connect((leader1 + 0) % servers)
	cfg.connect((leader1 + 1) % servers)
	cfg.connect(other)
	fmt.Printf("[Test] Disconnect everyone. Reconnect Leader: %d, Node: %d, %d. \n", (leader1+0)%servers, (leader1+1)%servers, other)
	fmt.Printf("[Test] Leader: %d should step down. Server %d should become leader and replicate its logs to servers %d, %d \n", (leader1+0)%servers, other, (leader1+0)%servers, (leader1+1)%servers)

	// Lots of successful commands to new group.
	for i := 0; i < 50; i++ {
		cfg.one(rand.Int(), 3, true)
	}
	fmt.Printf("[Test] Finished sending 4th batch of 50 commands to revived leader %d. These should be committed to nodes %d and %d.\n", leader1, (leader1+1)%servers, other)

	fmt.Printf("[Test] Reconnecting all servers. Sending one final command — all logs should be identical and fully committed.\n")
	// Now everyone.
	for i := 0; i < servers; i++ {
		cfg.connect(i)
	}
	cfg.one(rand.Int(), servers, true)

	fmt.Printf("[Test] Finish sedning one final command — all logs should be identical and fully committed.\n")

	cfg.end()
}

// yes
func TestCount3B(t *testing.T) {
	servers := 3
	cfg := make_config(t, servers, false, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): RPC counts aren't too high")

	rpcs := func() (n int) {
		for j := 0; j < servers; j++ {
			n += cfg.rpcCount(j)
		}
		return
	}

	leader := cfg.checkOneLeader()

	total1 := rpcs()

	if total1 > 30 || total1 < 1 {
		t.Fatalf("too many or few RPCs (%v) to elect initial leader\n", total1)
	}

	var total2 int
	var success bool
loop:
	for try := 0; try < 5; try++ {
		if try > 0 {
			// Give solution some time to settle.
			time.Sleep(3 * time.Second)
		}

		leader = cfg.checkOneLeader()
		total1 = rpcs()

		iters := 10
		starti, term, ok := cfg.rafts[leader].Start(1)
		if !ok {
			// Leader moved on really quickly.
			continue
		}
		cmds := []int{}
		for i := 1; i < iters+2; i++ {
			x := int(rand.Int31())
			cmds = append(cmds, x)
			index1, term1, ok := cfg.rafts[leader].Start(x)
			if term1 != term {
				// Term changed while starting.
				continue loop
			}
			if !ok {
				// No longer the leader, so term has changed.
				continue loop
			}
			if starti+i != index1 {
				t.Fatalf("Start() failed")
			}
		}

		for i := 1; i < iters+1; i++ {
			sameTerm, e := cfg.wait(starti+i, servers, term)
			if !sameTerm {
				// Term changed -- try again.
				continue loop
			}
			if e != (logEntry{true, cmds[i-1]}) {
				t.Fatalf("wrong value %+v committed for index %v; expected %+v\n",
					e, starti+i, logEntry{true, cmds[i-1]})
			}
		}

		failed := false
		total2 = 0
		for j := 0; j < servers; j++ {
			if t, _ := cfg.rafts[j].GetState(); t != term {
				// Term changed -- can't expect low RPC counts
				// need to keep going to update total2.
				failed = true
			}
			total2 += cfg.rpcCount(j)
		}

		if failed {
			continue loop
		}

		if total2-total1 > (iters+1+3)*3 {
			t.Fatalf("too many RPCs (%v) for %v entries\n", total2-total1, iters)
		}

		success = true
		break
	}

	if !success {
		t.Fatalf("term changed too often")
	}

	time.Sleep(RaftElectionTimeout)

	total3 := 0
	for j := 0; j < servers; j++ {
		total3 += cfg.rpcCount(j)
	}

	if total3-total2 > 3*20 {
		t.Fatalf("too many RPCs (%v) for 1 second of idleness\n", total3-total2)
	}

	cfg.end()
}

// yes
func TestUnreliableAgree3B(t *testing.T) {
	servers := 5
	cfg := make_config(t, servers, true, false)
	defer cfg.cleanup()

	cfg.begin("Test (3B): unreliable agreement")

	var wg sync.WaitGroup

	for iters := 1; iters < 50; iters++ {
		for j := 0; j < 4; j++ {
			wg.Add(1)
			go func(iters, j int) {
				defer wg.Done()
				cfg.one((100*iters)+j, 1, true)
			}(iters, j)
		}
		cfg.one(iters, 1, true)
	}

	cfg.setunreliable(false)

	wg.Wait()

	cfg.one(100, servers, true)

	cfg.end()
}
